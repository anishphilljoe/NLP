{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional, Flatten, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import chakin\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "embed_size=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"]\n",
    "list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix():\n",
    "        word2vecDict = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        embed_size = 300\n",
    "        \n",
    "        embeddings_index = dict()\n",
    "        for word in word2vecDict.wv.vocab:\n",
    "            embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "        print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            \n",
    "        gc.collect()\n",
    "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "        #same statistics for the rest of our own random generated weights. \n",
    "        all_embs = np.stack(list(embeddings_index.values()))\n",
    "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "        \n",
    "        nb_words = len(tokenizer.word_index)\n",
    "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "        #the size will be Number of Words in Vocab X Embedding Size\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "        gc.collect()\n",
    "\n",
    "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "        #our own dictionary and loaded pretrained embedding. \n",
    "        embeddedCount = 0\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            i-=1\n",
    "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            #and store inside the embedding matrix that we will train later on.\n",
    "            if embedding_vector is not None: \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                embeddedCount+=1\n",
    "        print('total embedded:',embeddedCount,'common words')\n",
    "        \n",
    "        del(embeddings_index)\n",
    "        gc.collect()\n",
    "        \n",
    "        #finally, return the embedding matrix\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishphilljoe/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Users/anishphilljoe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000000 word vectors.\n",
      "total embedded: 66078 common words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = loadEmbeddingMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 300)          63101100  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 120)          173280    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 63,280,736\n",
      "Trainable params: 179,636\n",
      "Non-trainable params: 63,101,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143613/143613 [==============================] - 2202s 15ms/step - loss: 0.0826 - acc: 0.9747 - val_loss: 0.0614 - val_acc: 0.9787\n",
      "Epoch 2/4\n",
      "143613/143613 [==============================] - 2824s 20ms/step - loss: 0.0571 - acc: 0.9804 - val_loss: 0.0541 - val_acc: 0.9809\n",
      "Epoch 3/4\n",
      "143613/143613 [==============================] - 2446s 17ms/step - loss: 0.0512 - acc: 0.9819 - val_loss: 0.0514 - val_acc: 0.9816\n",
      "Epoch 4/4\n",
      "143613/143613 [==============================] - 2093s 15ms/step - loss: 0.0477 - acc: 0.9827 - val_loss: 0.0521 - val_acc: 0.9816\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "hist = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(X_te)\n",
    "y = np.round(y,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_df = pd.read_csv('test_labels.csv')\n",
    "test_pred_df = pd.DataFrame(data=y, columns=['toxic_pred','severe_toxic_pred','obscene_pred','threat_pred','insult_pred','identity_hate_pred'])\n",
    "test_df = pd.concat([test_label_df,test_pred_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = tokenizer.word_index\n",
    "dum = {k:v for k,v in dum.items()}\n",
    "id_to_word = {value:key for key,value in dum.items()}\n",
    "id_to_word[0]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = []\n",
    "for LIST in X_te:\n",
    "    comment.append(' '.join(id_to_word[id] for id in LIST))\n",
    "test_df['comment_text'] = comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_df[test_df['toxic']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "\n",
      "6932\n",
      "Precission = 64.73%\n",
      "Accuracy = 93.67%\n",
      "Recall = 73.68%\n",
      "\n",
      "\n",
      "severe_toxic\n",
      "\n",
      "397\n",
      "Precission = 33.25%\n",
      "Accuracy = 99.22%\n",
      "Recall = 35.97%\n",
      "\n",
      "\n",
      "obscene\n",
      "\n",
      "3859\n",
      "Precission = 67.09%\n",
      "Accuracy = 96.29%\n",
      "Recall = 70.14%\n",
      "\n",
      "\n",
      "threat\n",
      "\n",
      "91\n",
      "Precission = 53.85%\n",
      "Accuracy = 99.68%\n",
      "Recall = 23.22%\n",
      "\n",
      "\n",
      "insult\n",
      "\n",
      "2542\n",
      "Precission = 73.17%\n",
      "Accuracy = 96.48%\n",
      "Recall = 54.27%\n",
      "\n",
      "\n",
      "identity_hate\n",
      "\n",
      "246\n",
      "Precission = 76.02%\n",
      "Accuracy = 99.09%\n",
      "Recall = 26.26%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = test_df[test_df['toxic']!=-1]\n",
    "field_LIST = [['toxic_pred','toxic'],\n",
    "['severe_toxic_pred','severe_toxic'],\n",
    "['obscene_pred','obscene'],\n",
    "['threat_pred','threat'],\n",
    "['insult_pred','insult'],\n",
    "['identity_hate_pred','identity_hate']]\n",
    "\n",
    "for field_LIST_ITEM in field_LIST:\n",
    "    print(field_LIST_ITEM[1]+'\\n')\n",
    "    print(len(data[(data[field_LIST_ITEM[0]]==1)]))\n",
    "    TP = len(data[(data[field_LIST_ITEM[0]]==1) & (data[field_LIST_ITEM[1]]==1)])\n",
    "    FP = len(data[(data[field_LIST_ITEM[0]]==1) & (data[field_LIST_ITEM[1]]==0)])\n",
    "    TN = len(data[(data[field_LIST_ITEM[0]]==0) & (data[field_LIST_ITEM[1]]==0)])\n",
    "    FN = len(data[(data[field_LIST_ITEM[0]]==0) & (data[field_LIST_ITEM[1]]==1)])\n",
    "    #print(\"TP: \"+str(TP))\n",
    "    #print(\"FP: \"+str(FP))\n",
    "    #print(\"TN: \"+str(TN))\n",
    "    #print(\"FN: \"+str(FN))\n",
    "    #print('Total test records: '+str(len(data)))\n",
    "    P = TP/(TP+FP)\n",
    "    A = (TP+TN)/(TP+TN+FP+FN)\n",
    "    R = TP/(TP+FN)\n",
    "    print('Precission = '+str(round(P*100,2))+'%')\n",
    "    print('Accuracy = '+str(round(A*100,2))+'%')\n",
    "    print('Recall = '+str(round(R*100,2))+'%\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer1 = Tokenizer(num_words=max_features)\n",
    "tokenizer1.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train1 = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test1 = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "X_t1 = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te1 = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix1():\n",
    "        word2vecDict = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        embed_size = 300\n",
    "        \n",
    "        embeddings_index = dict()\n",
    "        for word in word2vecDict.wv.vocab:\n",
    "            embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "        print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            \n",
    "        gc.collect()\n",
    "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "        #same statistics for the rest of our own random generated weights. \n",
    "        all_embs = np.stack(list(embeddings_index.values()))\n",
    "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "        \n",
    "        nb_words = len(tokenizer1.word_index)\n",
    "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "        #the size will be Number of Words in Vocab X Embedding Size\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "        gc.collect()\n",
    "\n",
    "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "        #our own dictionary and loaded pretrained embedding. \n",
    "        embeddedCount = 0\n",
    "        for word, i in tokenizer1.word_index.items():\n",
    "            i-=1\n",
    "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            #and store inside the embedding matrix that we will train later on.\n",
    "            if embedding_vector is not None: \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                embeddedCount+=1\n",
    "        print('total embedded:',embeddedCount,'common words')\n",
    "        \n",
    "        del(embeddings_index)\n",
    "        gc.collect()\n",
    "        \n",
    "        #finally, return the embedding matrix\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishphilljoe/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Users/anishphilljoe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000000 word vectors.\n",
      "total embedded: 66078 common words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = loadEmbeddingMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = Input(shape=(maxlen, ))\n",
    "x1 = Embedding(len(tokenizer1.word_index), embedding_matrix1.shape[1],weights=[embedding_matrix1],trainable=False)(inp1)\n",
    "x1 = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x1)\n",
    "x1 = GlobalMaxPool1D()(x1)\n",
    "#x1 = Dropout(0.1)(x1)\n",
    "x1 = Dense(50, activation=\"relu\")(x1)\n",
    "#x1 = Dropout(0.1)(x1)\n",
    "x1 = Dense(6, activation=\"sigmoid\")(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(inputs=inp1, outputs=x1)\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model1.summary()\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "with tf.device('/device:GPU:2'):\n",
    "    hist1 = model1.fit(X_t1,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = model1.predict(X_te1)\n",
    "y1 = np.round(y1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df1 = pd.DataFrame(data=y1, columns=['toxic_pred','severe_toxic_pred','obscene_pred','threat_pred','insult_pred','identity_hate_pred'])\n",
    "test_df1 = pd.concat([test_label_df,test_pred_df1], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_df1[test_df['toxic']!=-1]\n",
    "field_LIST = [['toxic_pred','toxic'],\n",
    "['severe_toxic_pred','severe_toxic'],\n",
    "['obscene_pred','obscene'],\n",
    "['threat_pred','threat'],\n",
    "['insult_pred','insult'],\n",
    "['identity_hate_pred','identity_hate']]\n",
    "\n",
    "for field_LIST_ITEM in field_LIST:\n",
    "    print(field_LIST_ITEM[1]+'\\n')\n",
    "    TP = len(data[(data[field_LIST_ITEM[0]]==1) & (data[field_LIST_ITEM[1]]==1)])\n",
    "    FP = len(data[(data[field_LIST_ITEM[0]]==1) & (data[field_LIST_ITEM[1]]==0)])\n",
    "    TN = len(data[(data[field_LIST_ITEM[0]]==0) & (data[field_LIST_ITEM[1]]==0)])\n",
    "    FN = len(data[(data[field_LIST_ITEM[0]]==0) & (data[field_LIST_ITEM[1]]==1)])\n",
    "    #print(\"TP: \"+str(TP))\n",
    "    #print(\"FP: \"+str(FP))\n",
    "    #print(\"TN: \"+str(TN))\n",
    "    #print(\"FN: \"+str(FN))\n",
    "    #print('Total test records: '+str(len(data)))\n",
    "    P = TP/(TP+FP)\n",
    "    A = (TP+TN)/(TP+TN+FP+FN)\n",
    "    R = TP/(TP+FN)\n",
    "    print('Precission = '+str(round(P*100,2))+'%')\n",
    "    print('Accuracy = '+str(round(A*100,2))+'%')\n",
    "    print('Recall = '+str(round(R*100,2))+'%\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 100, 300)          63101100  \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 100, 60)           86640     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 36006     \n",
      "=================================================================\n",
      "Total params: 63,223,746\n",
      "Trainable params: 122,646\n",
      "Non-trainable params: 63,101,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp1 = Input(shape=(maxlen, ))\n",
    "x1 = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp1)\n",
    "x1 = LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(6, activation=\"sigmoid\")(x1)\n",
    "model1 = Model(inputs=inp1, outputs=x1)\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 566s 4ms/step - loss: 0.0932 - acc: 0.9724 - val_loss: 0.0769 - val_acc: 0.9759\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 577s 4ms/step - loss: 0.0663 - acc: 0.9787 - val_loss: 0.0628 - val_acc: 0.9791\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 735s 5ms/step - loss: 0.0557 - acc: 0.9811 - val_loss: 0.0600 - val_acc: 0.9798\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 3\n",
    "hist1 = model1.fit(X_t1,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 100, 300)          63101100  \n",
      "_________________________________________________________________\n",
      "rnn_layer (SimpleRNN)        (None, 100, 60)           21660     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 36006     \n",
      "=================================================================\n",
      "Total params: 63,158,766\n",
      "Trainable params: 57,666\n",
      "Non-trainable params: 63,101,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp2 = Input(shape=(maxlen, ))\n",
    "x2 = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp2)\n",
    "x2 = SimpleRNN(60, return_sequences=True,name='rnn_layer',dropout=0.1,recurrent_dropout=0.1)(x2)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(6, activation=\"sigmoid\")(x2)\n",
    "model2 = Model(inputs=inp2, outputs=x2)\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 210s 1ms/step - loss: 0.1158 - acc: 0.9665 - val_loss: 0.1030 - val_acc: 0.9691\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 206s 1ms/step - loss: 0.1002 - acc: 0.9702 - val_loss: 0.1016 - val_acc: 0.9699\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 202s 1ms/step - loss: 0.0934 - acc: 0.9717 - val_loss: 0.0975 - val_acc: 0.9719\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 3\n",
    "hist2 = model2.fit(X_t1,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = model1.predict(X_te1)\n",
    "y1 = np.round(y1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df1 = pd.DataFrame(data=y1, columns=['toxic_pred','severe_toxic_pred','obscene_pred','threat_pred','insult_pred','identity_hate_pred'])\n",
    "test_df1 = pd.concat([test_label_df,test_pred_df1], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "\n",
      "Precission = 62.85%\n",
      "Accuracy = 93.19%\n",
      "Recall = 69.7%\n",
      "\n",
      "\n",
      "severe_toxic\n",
      "\n",
      "Precission = 36.44%\n",
      "Accuracy = 99.33%\n",
      "Recall = 22.34%\n",
      "\n",
      "\n",
      "obscene\n",
      "\n",
      "Precission = 73.14%\n",
      "Accuracy = 96.47%\n",
      "Recall = 61.37%\n",
      "\n",
      "\n",
      "threat\n",
      "\n",
      "Precission = 51.72%\n",
      "Accuracy = 99.67%\n",
      "Recall = 21.33%\n",
      "\n",
      "\n",
      "insult\n",
      "\n",
      "Precission = 69.9%\n",
      "Accuracy = 96.21%\n",
      "Recall = 51.5%\n",
      "\n",
      "\n",
      "identity_hate\n",
      "\n",
      "Precission = 56.7%\n",
      "Accuracy = 98.93%\n",
      "Recall = 15.45%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = test_df1[test_df1['toxic']!=-1]\n",
    "field_LIST = [['toxic_pred','toxic'],\n",
    "['severe_toxic_pred','severe_toxic'],\n",
    "['obscene_pred','obscene'],\n",
    "['threat_pred','threat'],\n",
    "['insult_pred','insult'],\n",
    "['identity_hate_pred','identity_hate']]\n",
    "\n",
    "for field_LIST_ITEM in field_LIST:\n",
    "    print(field_LIST_ITEM[1]+'\\n')\n",
    "    TP = len(data[(data[field_LIST_ITEM[0]]==1) & (data[field_LIST_ITEM[1]]==1)])\n",
    "    FP = len(data[(data[field_LIST_ITEM[0]]==1) & (data[field_LIST_ITEM[1]]==0)])\n",
    "    TN = len(data[(data[field_LIST_ITEM[0]]==0) & (data[field_LIST_ITEM[1]]==0)])\n",
    "    FN = len(data[(data[field_LIST_ITEM[0]]==0) & (data[field_LIST_ITEM[1]]==1)])\n",
    "    #print(\"TP: \"+str(TP))\n",
    "    #print(\"FP: \"+str(FP))\n",
    "    #print(\"TN: \"+str(TN))\n",
    "    #print(\"FN: \"+str(FN))\n",
    "    #print('Total test records: '+str(len(data)))\n",
    "    P = TP/(TP+FP)\n",
    "    A = (TP+TN)/(TP+TN+FP+FN)\n",
    "    R = TP/(TP+FN)\n",
    "    print('Precission = '+str(round(P*100,2))+'%')\n",
    "    print('Accuracy = '+str(round(A*100,2))+'%')\n",
    "    print('Recall = '+str(round(R*100,2))+'%\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = model2.predict(X_te1)\n",
    "y2 = np.round(y2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df2 = pd.DataFrame(data=y2, columns=['toxic_pred','severe_toxic_pred','obscene_pred','threat_pred','insult_pred','identity_hate_pred'])\n",
    "test_df2 = pd.concat([test_label_df,test_pred_df2], axis=1, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "\n",
      "Precission = 61.44%\n",
      "Accuracy = 91.77%\n",
      "Recall = 36.47%\n",
      "\n",
      "\n",
      "severe_toxic\n",
      "\n",
      "Precission = 35.82%\n",
      "Accuracy = 99.3%\n",
      "Recall = 27.52%\n",
      "\n",
      "\n",
      "obscene\n",
      "\n",
      "Precission = 62.78%\n",
      "Accuracy = 95.19%\n",
      "Recall = 40.94%\n",
      "\n",
      "\n",
      "threat\n",
      "\n",
      "Precission = 17.57%\n",
      "Accuracy = 99.6%\n",
      "Recall = 6.16%\n",
      "\n",
      "\n",
      "insult\n",
      "\n",
      "Precission = 53.25%\n",
      "Accuracy = 94.89%\n",
      "Recall = 37.82%\n",
      "\n",
      "\n",
      "identity_hate\n",
      "\n",
      "Precission = 27.52%\n",
      "Accuracy = 98.81%\n",
      "Recall = 4.21%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = test_df2[test_df2['toxic']!=-1]\n",
    "field_LIST = [['toxic_pred','toxic'],\n",
    "['severe_toxic_pred','severe_toxic'],\n",
    "['obscene_pred','obscene'],\n",
    "['threat_pred','threat'],\n",
    "['insult_pred','insult'],\n",
    "['identity_hate_pred','identity_hate']]\n",
    "\n",
    "for field_LIST_ITEM in field_LIST:\n",
    "    print(field_LIST_ITEM[1]+'\\n')\n",
    "    TP = len(data[(data[field_LIST_ITEM[0]]==1) & (data[field_LIST_ITEM[1]]==1)])\n",
    "    FP = len(data[(data[field_LIST_ITEM[0]]==1) & (data[field_LIST_ITEM[1]]==0)])\n",
    "    TN = len(data[(data[field_LIST_ITEM[0]]==0) & (data[field_LIST_ITEM[1]]==0)])\n",
    "    FN = len(data[(data[field_LIST_ITEM[0]]==0) & (data[field_LIST_ITEM[1]]==1)])\n",
    "    #print(\"TP: \"+str(TP))\n",
    "    #print(\"FP: \"+str(FP))\n",
    "    #print(\"TN: \"+str(TN))\n",
    "    #print(\"FN: \"+str(FN))\n",
    "    #print('Total test records: '+str(len(data)))\n",
    "    P = TP/(TP+FP)\n",
    "    A = (TP+TN)/(TP+TN+FP+FN)\n",
    "    R = TP/(TP+FN)\n",
    "    print('Precission = '+str(round(P*100,2))+'%')\n",
    "    print('Accuracy = '+str(round(A*100,2))+'%')\n",
    "    print('Recall = '+str(round(R*100,2))+'%\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
